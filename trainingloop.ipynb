{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/triston/Desktop/School/UVICAI/cucai/WildfireManagementRL/WildfireManagementRL/FireMap.py:164: RuntimeWarning: invalid value encountered in true_divide\n",
      "  adjusted_intensity_jump = np.where(non_zero_neighbours > 0, intensity_of_neighbours/non_zero_neighbours *FIRST_JUMP_RATIO*(1 - moisture * MOISTURE_INTENSITY_DAMP), 0)\n",
      "/tmp/ipykernel_28981/1032638561.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  PPO_Agent.memory.store_memory(prev_obs, T.tensor(actions), logprob, advantage, prev_vf, rewards, dones)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int64' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m next_vf \u001b[38;5;241m=\u001b[39m next_vf\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     68\u001b[0m advantage \u001b[38;5;241m=\u001b[39m PPO_Agent\u001b[38;5;241m.\u001b[39mget_gae(rewards, prev_vf, next_vf)\n\u001b[0;32m---> 69\u001b[0m \u001b[43mPPO_Agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madvantage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_vf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Learning loop\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(PPO_Agent\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mstates) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m NUM_MINIBATCHES\u001b[38;5;241m*\u001b[39mMINIBATCH:\n",
      "File \u001b[0;32m~/Desktop/School/UVICAI/cucai/WildfireManagementRL/WildfireManagementRL/PPO_Agent_Misc/PPOContinuous.py:31\u001b[0m, in \u001b[0;36mPPOMemory.store_memory\u001b[0;34m(self, state, action, probs, adv, vals, reward, done)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madv\u001b[38;5;241m.\u001b[39mappend(adv\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvals\u001b[38;5;241m.\u001b[39mappend(vals\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[43mreward\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m())\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdones\u001b[38;5;241m.\u001b[39mappend(done)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.int64' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "from PPO_Agent_Misc.PPOContinuous import Agent as ContPPO\n",
    "from Simulation import FireMapEnv\n",
    "from odds_and_ends.data_processing import process_RL_outputs, process_env_for_agent\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "EPOCHS = 25\n",
    "NUM_MINIBATCHES = 32\n",
    "MINIBATCH = 16\n",
    "EPISODES = 100\n",
    "TS_PER_ITER = 2000\n",
    "\n",
    "# Continuous Parameters\n",
    "action_min = T.tensor((0.0, -1.0))\n",
    "action_max = T.tensor((1.0, 1.0))\n",
    "\n",
    "PPO_Agent = ContPPO(n_actions=6, c1=1.0, c2=0.5, input_dims=7200, action_min=action_min, action_max=action_max, \n",
    "                    gamma=0.99, gae_lambda=0.95, policy_clip=0.2, batch_size=MINIBATCH, \n",
    "                    buffer_size=MINIBATCH*NUM_MINIBATCHES, n_epochs=EPISODES, LR=1e-3, annealing=False)\n",
    "\n",
    "env = FireMapEnv()\n",
    "env.reset()\n",
    "\n",
    "actions = np.reshape(np.random.randint(0, 45/3, 6), (3,2))\n",
    "obs, rewards, dones, info = env.step(actions)\n",
    "\n",
    "full_episode_loss = []\n",
    "avg_policy_loss = []\n",
    "avg_crit_loss = []\n",
    "episode_max_ratio = []\n",
    "ep_mean_rewards = []\n",
    "\n",
    "obs = T.tensor(obs).float().to(PPO_Agent.device)\n",
    "obs = process_env_for_agent(obs)\n",
    "obs = T.flatten(obs)\n",
    "\n",
    "env.reset()\n",
    "for _ in range(EPISODES):\n",
    "    # Step one, get some sort of training running. \n",
    "    for _ in range(TS_PER_ITER):\n",
    "        # Set prev observation\n",
    "        prev_obs = obs.clone()\n",
    "        \n",
    "        # Get actions from the agent\n",
    "        actions, logprob, mean, prev_vf = PPO_Agent.get_action_and_vf(prev_obs)\n",
    "\n",
    "        actions += 15\n",
    "        \n",
    "        # Process raw actions into the environment \n",
    "        processed_actions = process_RL_outputs(actions.numpy())\n",
    "        obs, rewards, dones, _ = env.step(processed_actions)\n",
    "        obs = T.tensor(obs).to(PPO_Agent.device)\n",
    "        obs = process_env_for_agent(obs)\n",
    "        obs = T.flatten(obs).float()  \n",
    "\n",
    "        # Get value of obs and store in agent\n",
    "        next_vf = PPO_Agent.critic.forward(obs)\n",
    "        advantage = PPO_Agent.get_gae(rewards, prev_vf, next_vf)\n",
    "        PPO_Agent.memory.store_memory(prev_obs, T.tensor(actions), logprob, advantage, prev_vf, rewards, dones)\n",
    "\n",
    "        # Learning loop\n",
    "        if len(PPO_Agent.memory.states) >= NUM_MINIBATCHES*MINIBATCH:\n",
    "            for _ in range(NUM_MINIBATCHES):\n",
    "                print(\"REWARDS \", PPO_Agent.memory.rewards)\n",
    "                rewards = T.tensor(PPO_Agent.memory.rewards, dtype=T.float32)\n",
    "                rew_mean = rewards.mean()\n",
    "                rew_std = rewards.std()\n",
    "                e_policy_loss, e_crit_loss, loss = PPO_Agent.learn(rew_mean, rew_std)\n",
    "\n",
    "            env.render()\n",
    "\n",
    "        PPO_Agent.c2 *= 0.95\n",
    "        PPO_Agent.actor.var *= 0.95\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0022, 0.0000, 0.1405, 0.0790, 0.0730]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ActorModel(nn.Module):\n",
    "    '''\n",
    "        A continuous actionspace forces us to create some sort of distribution\n",
    "        on the range of values on our actionspace. We force a normal distribution.\n",
    "        Functionally what this means is our mean and var values are parameters \n",
    "        that change with training, so we have to create some sort of basis for this.\n",
    "        \n",
    "        Variational Autoencoders split a layer at the same depth to account for \n",
    "        this exact same thing. The code will look similar.\n",
    "\n",
    "    '''\n",
    "    def __init__(self, input_shape, n_actions, \n",
    "                 min_tens = T.tensor((-1, 1)), max_tens = T.tensor((-1, 1))):\n",
    "        super(ActorModel, self).__init__()\n",
    "\n",
    "        # base model\n",
    "        self.conv1 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, padding=0, stride=2).to(device)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, padding=0, stride=2).to(device)\n",
    "        self.flatten_layer = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(in_features=1568, out_features=64).to(device)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=64).to(device)\n",
    "\n",
    "        # distributions\n",
    "        self.mean = nn.Linear(in_features=64, out_features=n_actions).to(device)\n",
    "\n",
    "        # Constant variance\n",
    "        self.var = T.diag(T.ones(n_actions)).to(device)*20\n",
    "\n",
    "        # misc\n",
    "        self.min_tens = min_tens\n",
    "        self.max_tens = max_tens\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' We create a class that computes the distribution of our actions. '''\n",
    "\n",
    "        # base computation\n",
    "        x = F.relu(self.conv2(x)).to(device)\n",
    "        x = self.flatten_layer(x)\n",
    "        x = F.relu(self.fc1(x)).to(device)\n",
    "        x = F.relu(self.fc2(x)).to(device)\n",
    "\n",
    "        # split\n",
    "        # we note the mean value goes through a tanh as it corresponds with the \n",
    "        # lunar lander task at hand. Specifically, we have a output which ranges from (-1,1)\n",
    "        mean = F.relu(self.mean(x)).to(device)\n",
    "\n",
    "        return mean\n",
    "    \n",
    "test = ActorModel(4, 5)\n",
    "test.forward(obs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
